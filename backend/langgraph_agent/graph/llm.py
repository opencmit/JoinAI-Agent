import copy

from langgraph_agent.config import global_config

import os
import asyncio
import logging
import re

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig

from langgraph_agent.graph.state import AgentState

from copilotkit.langgraph import copilotkit_customize_config

try:
    import httpx
except ImportError:
    httpx = None



logger = logging.getLogger(__name__)

def get_llm_client(state: AgentState, config: RunnableConfig) -> (ChatOpenAI, str):
    """è·å–æˆ–åˆ›å»ºLLMå®¢æˆ·ç«¯ï¼ˆæ— ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    print("\n=== è·å– LLM å®¢æˆ·ç«¯ ===")

    # model_name = (
    #     state.get("model")
    #     if "model" in state and state.get("model") is not None
    #     else config.get("configurable", {}).get("model_name", global_config.BASE_LLM)
    # )

    model_name = os.getenv("BASE_LLM", "deepseek-ai/DeepSeek-V3")
    # print("model_name:{}".format(model_name))

    # openai_api_key = config.get("configurable", {}).get("model_key", os.getenv("OPENAI_API_KEY"))
    openai_api_key = os.getenv("OPENAI_API_KEY")
    # print("openai_api_key:{}".format(openai_api_key))
    base_url = os.getenv("OPENAI_BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")

    if not model_name:
        model_name = global_config.BASE_LLM

    print(f"æ¨¡å‹åç§°: {model_name}")
    print(f"Base URL: {base_url}")
    print(f"åˆ›å»ºæ–°çš„ LLM å®¢æˆ·ç«¯ï¼ˆæ— ç¼“å­˜ï¼‰...")

    try:
        # é…ç½® SSL éªŒè¯ï¼ˆé»˜è®¤å¯ç”¨ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡ç¦ç”¨ï¼‰
        ssl_verify = os.getenv('OPENAI_SSL_VERIFY', 'true').lower() == 'true'
        
        # åˆ›å»º httpx å®¢æˆ·ç«¯é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        http_client_kwargs = {}
        if httpx is not None:
            # ChatOpenAI æœŸæœ›åŒæ­¥ç‰ˆ httpx.Clientï¼Œä¼  AsyncClient ä¼šè§¦å‘ç±»å‹æ ¡éªŒé”™è¯¯
            http_client = httpx.Client(
                verify=ssl_verify,
                timeout=int(os.getenv('OPENAI_REQUEST_TIMEOUT', '300')),
                limits=httpx.Limits(
                    max_keepalive_connections=5,
                    max_connections=10
                )
            )
            http_client_kwargs['http_client'] = http_client
        
        # print("base_url:{}".format(base_url))
        # print("model_name:{}".format(model_name))
        # print("openai_api_key:{}".format(openai_api_key))
        client = ChatOpenAI(
            base_url=base_url,
            model=model_name,
            temperature=0.1,
            api_key=openai_api_key,
            model_kwargs={
                "extra_headers": {
                    "Authorization": f"Bearer {openai_api_key}"
                }
            },
            request_timeout=int(os.getenv('OPENAI_REQUEST_TIMEOUT', '300')),
            max_retries=int(os.getenv('OPENAI_MAX_RETRIES', '5')),
            **http_client_kwargs
        )
        
        
        print(f"[LLMå®¢æˆ·ç«¯] âœ… åˆ›å»ºæ–°çš„å®¢æˆ·ç«¯æˆåŠŸï¼Œæ¨¡å‹: {model_name}")
        return client, model_name

    except Exception as e:
        print(f"[LLMå®¢æˆ·ç«¯] âŒ åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        raise


def build_messages_for_llm(model_name, messages):
    messages_for_llm = []


    # for msg in messages:
    #     if hasattr(msg, 'content'):
    #         # ç¡®ä¿å†…å®¹å¯ä»¥è¢«JSONåºåˆ—åŒ–
    #         try:
    #             json.dumps({"content": msg.content})
    #             cleaned_messages.append(msg)
    #         except:
    #             # å¦‚æœåºåˆ—åŒ–å¤±è´¥ï¼Œåˆ›å»ºæ¸…ç†åçš„æ¶ˆæ¯
    #             cleaned_content = sanitize_string_for_json(msg.content, context="api")
    #             if hasattr(msg, 'type'):
    #                 if msg.type == "system":
    #                     cleaned_messages.append(SystemMessage(content=cleaned_content))
    #                 elif msg.type == "human":
    #                     cleaned_messages.append(HumanMessage(content=cleaned_content))
    #                 elif msg.type == "ai":
    #                     ai_msg = AIMessage(content=cleaned_content)
    #                     # ä¿ç•™å·¥å…·è°ƒç”¨
    #                     if hasattr(msg, 'tool_calls') and msg.tool_calls:
    #                         ai_msg.tool_calls = msg.tool_calls
    #                     cleaned_messages.append(ai_msg)
    #     else:
    #         cleaned_messages.append(msg)

    # logger.info(f"Messages For LLM start: {messages}")

    for msg in messages:

        # éªŒè¯messageä¸­çš„nameå­—æ®µæ˜¯å¦è¢«è®¾ç½®
        if isinstance(msg, AIMessage) or isinstance(msg, ToolMessage):
            if not msg.name:
                logger.error("messageçš„nameå­—æ®µæœªè¢«æ­£ç¡®è®¾ç½®")
                logger.error(f"æœ‰é—®é¢˜çš„msg: {msg}")

            if isinstance(msg, AIMessage):
                if not msg.name == "a2a_agent":
                    msg.content = f"Response from {msg.name}: " + msg.content
                messages_for_llm.append(
                    HumanMessage(
                        content=msg.content,
                        name=msg.name,
                        tool_calls=msg.tool_calls if msg.tool_calls else [],
                        additional_kwargs=msg.additional_kwargs if msg.additional_kwargs else {}
                    ))

            elif isinstance(msg, ToolMessage):
                msg.content = f"Response from {msg.name} tool: " + msg.content
                messages_for_llm.append(
                    HumanMessage(
                        content=msg.content,
                        name=msg.name,
                        additional_kwargs=msg.additional_kwargs if msg.additional_kwargs else {}
                    ))

        else:
            messages_for_llm.append(msg)

    if model_name and "Qwen3-235B" in model_name:
        messages[0].content += ' /no_think'

    # logger.info(f"Messages For LLM: {messages_for_llm}")

    return messages_for_llm


async def safe_llm_invoke(llm, config: RunnableConfig, model_name, messages, max_retries=3, hidden=False, disable_emit=False):
    """å®‰å…¨çš„LLMè°ƒç”¨ï¼ŒåŒ…å«é‡è¯•é€»è¾‘"""
    for attempt in range(max_retries):
        try:
            # åœ¨è°ƒç”¨å‰å†æ¬¡éªŒè¯æ¶ˆæ¯æ ¼å¼
            messages_for_llm = build_messages_for_llm(model_name, copy.deepcopy(messages))
            # è°ƒç”¨LLM
            if hidden:
                config["tags"] = ["langsmith:hidden"]
                # mcpå·¥å…·åœºæ™¯ï¼Œéœ€ç¦ç”¨llmè‡ªåŠ¨emitã€‚å› ä¸ºæ­¤æ—¶messageä¸æ˜¯å®é™…è°ƒç”¨mcpå·¥å…·çš„å‚æ•°ã€‚
                modified_config = copilotkit_customize_config(
                    config,
                    emit_messages=False if disable_emit else True,  # if you want to disable message streaming #
                    emit_tool_calls=False  # if you want to disable tool call streaming #
                )
                # print(f"safe_llm_invoke: {modified_config}")
            else:
                modified_config = copilotkit_customize_config(
                    config,
                    emit_messages=False if disable_emit else True,  # if you want to disable message streaming #
                    emit_tool_calls=False  # if you want to disable tool call streaming #
                )

            response = await llm.ainvoke(messages_for_llm, config=modified_config)
            # print(f"safe_llm_invoke: {modified_config}")

            return response

        except Exception as e:
            error_msg = str(e)
            error_type = type(e).__name__
            logger.error(f"LLM invoke error: {error_msg}")
            
            # ç‰¹æ®Šå¤„ç† SSL é”™è¯¯
            if "SSL" in error_msg or "ssl" in error_msg or "SSLError" in error_type:
                print(f"æ£€æµ‹åˆ° SSL é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                if attempt < max_retries - 1:
                    # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•ï¼ŒSSL é”™è¯¯å¯èƒ½æ˜¯æš‚æ—¶çš„ç½‘ç»œé—®é¢˜
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œæä¾›è§£å†³å»ºè®®
                    raise Exception(
                        f"SSL è¿æ¥å¤±è´¥: {error_msg}\n"
                        "å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:\n"
                        "1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                        "2. æ£€æŸ¥ OPENAI_BASE_URL æ˜¯å¦æ­£ç¡®\n"
                        "3. å¦‚æœæ˜¯è‡ªç­¾åè¯ä¹¦ï¼Œå¯ä»¥è®¾ç½® OPENAI_SSL_VERIFY=falseï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰\n"
                        "4. æ£€æŸ¥é˜²ç«å¢™æˆ–ä»£ç†è®¾ç½®"
                    ) from e
            
            # ç‰¹æ®Šå¤„ç†è¿æ¥å…³é—­é”™è¯¯
            elif "Cannot send a request, as the client has been closed" in error_msg:
                print(f"æ£€æµ‹åˆ°å®¢æˆ·ç«¯å·²å…³é—­é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                # è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— æ³•åœ¨è¿™é‡Œé‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
                # å› ä¸ºå®¢æˆ·ç«¯æ˜¯åœ¨å¤–éƒ¨åˆ›å»ºçš„ï¼Œæ‰€ä»¥ç›´æ¥æŠ›å‡ºå¼‚å¸¸
                # è®©è°ƒç”¨æ–¹å¤„ç†
                raise Exception("LLMå®¢æˆ·ç«¯å·²å…³é—­ï¼Œéœ€è¦é‡æ–°åˆ›å»º") from e

            # å¤„ç†å…¶ä»–JSONç›¸å…³é”™è¯¯
            elif "delimiter" in error_msg or "JSON" in error_msg or "Expecting" in error_msg:
                print(f"JSONæ ¼å¼é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                if attempt < max_retries - 1:
                    # ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    await asyncio.sleep(1)
                    continue

            # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
            raise

    raise Exception("LLMè°ƒç”¨å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°")


LLM_ERROR_LIST = {
    "json": [
        "1. ç”¨æˆ·è¾“å…¥åŒ…å«ç‰¹æ®Šå­—ç¬¦æœªæ­£ç¡®è½¬ä¹‰",
        "2. å·¥å…·è°ƒç”¨å‚æ•°æ ¼å¼é”™è¯¯",
        "3. ç³»ç»Ÿpromptæˆ–æ¶ˆæ¯åŒ…å«æ ¼å¼é”™è¯¯çš„JSON",
        "4. ä¸­æ–‡å­—ç¬¦ç¼–ç é—®é¢˜",
    ],
    "connection": [
        "1. ç½‘ç»œè¿æ¥é—®é¢˜ - æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸",
        "2. API ç«¯ç‚¹ä¸å¯è®¿é—® - æ£€æŸ¥ Base URL æ˜¯å¦æ­£ç¡®",
        "3. é˜²ç«å¢™æˆ–ä»£ç†é—®é¢˜ - æ£€æŸ¥æ˜¯å¦æœ‰ç½‘ç»œé™åˆ¶",
        "4. SSL/TLS è¯ä¹¦é—®é¢˜ - å°è¯•ç¦ç”¨ SSL éªŒè¯ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰"
    ],
    "timeout": [
        "1. è¯·æ±‚è¶…æ—¶ - è€ƒè™‘å¢åŠ è¶…æ—¶æ—¶é—´",
        "2. æœåŠ¡å™¨å“åº”ç¼“æ…¢ - æ£€æŸ¥æœåŠ¡å™¨è´Ÿè½½",
        "3. ç½‘ç»œå»¶è¿Ÿé«˜ - æ£€æŸ¥ç½‘ç»œè´¨é‡"
    ],
    "authentication": [
        "1. API Key æ— æ•ˆæˆ–è¿‡æœŸ"
        "2. API Key æƒé™ä¸è¶³"
        "3. è®¤è¯å¤´æ ¼å¼é”™è¯¯"
    ],
    "rate limit": [
        "1. API è°ƒç”¨é¢‘ç‡è¶…é™",
        "2. å¹¶å‘è¯·æ±‚è¿‡å¤š",
        "3. é…é¢å·²ç”¨å®Œ"
    ],
    "other": [
        "1. API æœåŠ¡å¼‚å¸¸",
        "2. è¯·æ±‚æ ¼å¼é”™è¯¯",
        "3. æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨"
    ]
}


def get_error_msg(type: str) -> str:
    if type == "json":
        return "\nğŸ” æ£€æµ‹åˆ°JSONæ ¼å¼é”™è¯¯ï¼Œæ‰§è¡Œä¸“é¡¹å¤„ç†:\n" + "\n".join(LLM_ERROR_LIST[type])
    elif type in ("connection", "timeout", "authentication", "rate limit"):
        return "\nå¯èƒ½çš„åŸå› :\n" + "\n".join(LLM_ERROR_LIST[type])
    elif type == "other":
        return "\nå…¶ä»–å¯èƒ½çš„åŸå› :\n" + "\n".join(LLM_ERROR_LIST[type])
    else:
        return "æœªçŸ¥é”™è¯¯ç±»å‹"

def remove_text_between_delimiters(text, start_delim, end_delim):
    '''
    æœ¬å‡½æ•°ç”¨äºåˆ é™¤ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­ä¸¤ä¸ªæŒ‡å®šåˆ†éš”ç¬¦ä¹‹é—´çš„å†…å®¹ã€‚ç›®å‰ç”¨äºæ¸…é™¤qwen3-235bå›å¤å†…å®¹ä¸­çš„<think></think>æ ‡ç­¾
    Args:
        text: åŸå§‹å­—ç¬¦ä¸²
        start_delim: åˆ†éš”ç¬¦ï¼ˆå·¦ï¼‰
        end_delim: åˆ†éš”ç¬¦ï¼ˆå³ï¼‰
    Returns:
    '''
    start_escaped = re.escape(start_delim)
    end_escaped = re.escape(end_delim)
    pattern = f'{start_escaped}.*?{end_escaped}'
    return re.sub(pattern, '', text, flags=re.DOTALL)
