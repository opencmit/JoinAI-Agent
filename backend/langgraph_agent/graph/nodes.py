"""
å›¾çš„å„ä¸ªèŠ‚ç‚¹ã€ä»¥åŠèŠ‚ç‚¹ä¹‹é—´çš„è·¯ç”±å‡½æ•°
"""

import os
from copy import deepcopy
from typing import Dict, Any, Optional, List, Literal
import traceback
import json
import datetime
import time
import sys
import platform
import re
import asyncio
import logging
import requests
import json_repair

from langchain_openai import ChatOpenAI
from langchain.tools.base import BaseTool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_core.runnables import RunnableConfig, RunnableLambda
from langgraph.errors import GraphInterrupt
from contextlib import ExitStack
from copilotkit.langgraph import copilotkit_emit_state
from langgraph.types import Command
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

from langgraph_agent.utils.convert_md import convert_to_markdown_unified

from langgraph_agent.graph.state import AgentState
from langgraph_agent.utils.tool_utils import normalize_mcp_tool_data, normalize_tool_result
from langgraph_agent.utils.tool_utils import fix_markdown_display, process_raw_html_content
from langgraph_agent.utils.json_utils import sanitize_string_for_json, deep_clean_for_json, safe_json_dumps, validate_tool_calls_json, repair_json_output
from langgraph_agent.utils.message_utils import get_last_show_message_id
from langgraph_agent.graph.llm import get_llm_client, safe_llm_invoke, get_error_msg
from langgraph_agent.constant import TEAM_MEMBERS, RESPONSE_FORMAT
from langgraph_agent.graph.a2a_agent import get_a2a_agents_from_state


from typing import Dict, Any, Optional, List, Literal

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_core.runnables import RunnableConfig, RunnableLambda


from langgraph_agent.graph.state import AgentState
from langgraph_agent.utils.tool_utils import normalize_mcp_tool_data
from langgraph_agent.utils.tool_utils import get_attachment_tools
from langgraph_agent.utils.json_utils import sanitize_string_for_json, deep_clean_for_json
from langgraph_agent.graph.mcp_tool import create_mcp_converter_with_llm
from langgraph_agent.config import global_config


logger = logging.getLogger(__name__)

async def agent_node(
        state: AgentState,
        config: RunnableConfig,
        llm: Optional[ChatOpenAI],
        prompt: str,
        cur_node: str
) -> Dict:
    """Agent èŠ‚ç‚¹ - ä¿®å¤JSONæ ¼å¼é”™è¯¯ç‰ˆæœ¬ï¼Œå¢å¼ºå·¥å…·ä½¿ç”¨ç¤ºä¾‹"""
    if llm is None:
        raise ValueError("LLM is not initialized for agent_node")
        
    # logger.info("Agent Node Messages:\n{}".format(state["messages"]))

    print(f"\n=== Agent Node æ¶ˆæ¯è°ƒè¯• (JSONæ ¼å¼é”™è¯¯ä¿®å¤ç‰ˆæœ¬) ===")
    print(f"çŠ¶æ€ä¸­çš„æ¶ˆæ¯æ•°é‡: {len(state['messages'])}")

    # è®°å½•LLMé…ç½®ä¿¡æ¯
    print(f"\n=== LLM é…ç½®ä¿¡æ¯ ===")
    print(f"LLM ç±»å‹: {type(llm).__name__}")
    print(f"æ¨¡å‹åç§°: {getattr(llm, 'model_name', 'unknown')}")
    print(f"Base URL: {getattr(llm, 'openai_api_base', 'unknown')}")
    print(f"è¶…æ—¶è®¾ç½®: {getattr(llm, 'request_timeout', 'unknown')}")
    print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {getattr(llm, 'max_retries', 'unknown')}")

    # è·å–ç”¨æˆ·çš„æœ€æ–°æŸ¥è¯¢å†…å®¹å¹¶æ¸…ç†
    user_query = ""
    for msg in reversed(state["messages"]):
        if hasattr(msg, 'type') and msg.type == "human":
            user_query = msg.content
            break

    print(f"ç”¨æˆ·æœ€æ–°æŸ¥è¯¢ï¼ˆåŸå§‹ï¼‰: {user_query}")
    

    # ä½¿ç”¨deepcopyï¼Œä¸å½±å“state.messages
    llm_messages = deepcopy(state["messages"])
    # print("llm_messages:{}".format(llm_messages))

    if state.get("sub_task"):
        logger.info(f"user_queryæ”¹ä¸ºä½¿ç”¨å­ä»»åŠ¡: {state.get('sub_task')}")
        user_query = state.get("sub_task")

    llm_messages.append(HumanMessage(content=user_query))
    
    

    # åˆ›å»ºç³»ç»Ÿæ¶ˆæ¯ - æ¸…ç†ç³»ç»Ÿprompt
    # try:
    #     system_prompt = sanitize_string_for_json(prompt, context="api")
    #     system_message = SystemMessage(content=system_prompt)
    # except Exception as e:
    #     print(f"âš ï¸ ç³»ç»Ÿpromptåˆ›å»ºå¤±è´¥: {str(e)}")
    #     # ä½¿ç”¨ç®€åŒ–çš„ç³»ç»Ÿæ¶ˆæ¯
    #     simple_system_prompt = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
    #     system_message = SystemMessage(content=simple_system_prompt)
    #
    # llm_messages.insert(0, system_message)

    # æ”¹è¿›çš„å¾ªç¯æ£€æµ‹é€»è¾‘
    iteration_count = state.get("iteration_count", 0)

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    max_iterations = state.get("max_iterations", 50)
    if iteration_count >= max_iterations:
        print(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¼ºåˆ¶ç»“æŸ")
        state['completed'] = True
        # æ·»åŠ ç»“æŸæ¶ˆæ¯
        final_message = AIMessage(content="å¯¹è¯å·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè‡ªåŠ¨ç»“æŸã€‚å¦‚éœ€ç»§ç»­ï¼Œè¯·å¼€å§‹æ–°çš„å¯¹è¯ã€‚", name=cur_node)
        state["messages"].append(final_message)
        return {
            "completed": True,
            "messages": [final_message]
        }

    # æ£€æŸ¥æœ€è¿‘çš„AIæ¶ˆæ¯æ˜¯å¦é‡å¤
    recent_ai_messages = [msg for msg in state["messages"][-10:] if
                          hasattr(msg, 'type') and msg.type == "ai" and msg.content.strip()]
    if len(recent_ai_messages) >= 3:
        # æ£€æŸ¥æœ€è¿‘3æ¡AIæ¶ˆæ¯çš„å†…å®¹ç›¸ä¼¼åº¦
        last_three_contents = [msg.content.strip() for msg in recent_ai_messages[-3:]]

        # ç®€å•çš„é‡å¤æ£€æµ‹ï¼šå¦‚æœæœ€åä¸¤æ¡æ¶ˆæ¯å®Œå…¨ç›¸åŒ
        if len(set(last_three_contents)) == 1 and len(last_three_contents) >= 3:
            print("æ£€æµ‹åˆ°AIæ¶ˆæ¯é‡å¤ï¼Œå¼ºåˆ¶ç»“æŸå¯¹è¯")
            state['completed'] = True
            # æ·»åŠ ç»“æŸæ¶ˆæ¯
            final_message = AIMessage(content="æ£€æµ‹åˆ°å“åº”é‡å¤ï¼Œå¯¹è¯ç»“æŸã€‚å¦‚éœ€ç»§ç»­ï¼Œè¯·æä¾›æ›´å…·ä½“çš„è¯·æ±‚æˆ–å¼€å§‹æ–°çš„å¯¹è¯ã€‚", name=cur_node)
            state["messages"].append(final_message)
            return {
                "completed": True,
                "messages": [final_message]
            }

        # æ£€æŸ¥æ˜¯å¦éƒ½æ˜¯æ ‡å‡†ç»“æŸè¯­
        standard_endings = [
            "æˆ‘ç†è§£æ‚¨çš„è¯·æ±‚ã€‚è¯·é—®è¿˜æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
            "å¦‚æœä¸€åˆ‡æ¸…æ¥šï¼Œå¯ä»¥ç›´æ¥å‘Šè¯‰æˆ‘ï¼Œä»¥ä¾¿æˆ‘ä»¬ç»“æŸè¿™æ¬¡å¯¹è¯ã€‚"
        ]

        if all(content in standard_endings for content in last_three_contents[-3:]):
            print("æ£€æµ‹åˆ°è¿ç»­çš„æ ‡å‡†ç»“æŸè¯­ï¼Œå¼ºåˆ¶ç»“æŸå¯¹è¯")
            state['completed'] = True
            # æ·»åŠ ç»“æŸæ¶ˆæ¯
            final_message = AIMessage(content="å¯¹è¯è‡ªç„¶ç»“æŸã€‚æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼å¦‚æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶å¼€å§‹æ–°çš„å¯¹è¯ã€‚", name=cur_node)
            state["messages"].append(final_message)
            return {
                "completed": True,
                "messages": [final_message]
            }

    try:
        # è®°å½•LLMè°ƒç”¨ä¿¡æ¯
        print(f"\n=== LLM è°ƒç”¨è¯¦æƒ… ===")
        # è®°å½•è°ƒç”¨æ—¶é—´
        start_time = time.time()
        print(f"å¼€å§‹è°ƒç”¨ LLM: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")

        model_name = state.get("model")

        # è°ƒç”¨ LLM (ä½¿ç”¨å®‰å…¨è°ƒç”¨å‡½æ•°)
        response = await safe_llm_invoke(llm, config, model_name, llm_messages, disable_emit=True)

        logger.info(f"LLM å“åº”å†…å®¹ï¼š{response}")
        
        # è®°å½•å“åº”æ—¶é—´
        end_time = time.time()
        print(f"LLM å“åº”å®Œæˆ: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
        print(f"å“åº”è€—æ—¶: {end_time - start_time:.2f} ç§’")
        

        # éªŒè¯å·¥å…·è°ƒç”¨
        if hasattr(response, "tool_calls") and response.tool_calls:
            print("---------------------")
            print(f"LLM å“åº”åŒ…å«å·¥å…·è°ƒç”¨: {[tc['name'] for tc in response.tool_calls]}")

            # éªŒè¯å’Œæ¸…ç†å·¥å…·è°ƒç”¨å‚æ•°
            try:
                cleaned_tool_calls = validate_tool_calls_json(response.tool_calls)
                response.tool_calls = cleaned_tool_calls
                print("âœ… å·¥å…·è°ƒç”¨å‚æ•°éªŒè¯å’Œæ¸…ç†å®Œæˆ")
            except Exception as tc_e:
                print(f"âš ï¸ å·¥å…·è°ƒç”¨æ¸…ç†å¤±è´¥: {str(tc_e)}")
                # å¦‚æœå·¥å…·è°ƒç”¨æ¸…ç†å¤±è´¥ï¼Œç§»é™¤å·¥å…·è°ƒç”¨
                response.tool_calls = []
        else:
            print("LLM å“åº”ä¸åŒ…å«å·¥å…·è°ƒç”¨")
            if not response.content or not response.content.strip():
                print("è­¦å‘Šï¼šLLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨é»˜è®¤å›å¤")
                response.content = "æˆ‘ç†è§£æ‚¨çš„è¯·æ±‚ã€‚è¯·é—®è¿˜æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        
        # å¤„ç†å“åº”å†…å®¹
        if response.content and any(indicator in response.content.lower() for indicator in ['.md', 'markdown', '```']):
            # å¦‚æœå“åº”åŒ…å«Markdownå†…å®¹ï¼Œä¿®å¤æ˜¾ç¤º
            response.content = fix_markdown_display(response.content)

        response.name = cur_node
        state["messages"].append(response)
        state["inner_messages"].append(response)

        print(f"LLM å“åº”å†…å®¹: {response.content[:300]}...")
        

        # ğŸ”¥ æ–°å¢ï¼šå¤šæ­¥éª¤å·¥ä½œæµçŠ¶æ€æ›´æ–°é€»è¾‘ï¼ˆç±»ä¼¼A2Aæ™ºèƒ½ä½“ï¼‰
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ­¥éª¤å·¥ä½œæµä¸­çš„é€šç”¨æ™ºèƒ½ä½“æ‰§è¡Œ
        workflow_plan = state.get("workflow_plan") or {}
        workflow_type = workflow_plan.get("workflow_type", "single_step")
        current_step_index = state.get("current_step_index", 0)

        if workflow_type == "multi_step":
            print(f"ğŸ”„ é€šç”¨æ™ºèƒ½ä½“åœ¨å¤šæ­¥éª¤å·¥ä½œæµä¸­æ‰§è¡Œå®Œæˆï¼Œå½“å‰æ­¥éª¤: {current_step_index + 1}")

            # æ ‡è®°å½“å‰æ­¥éª¤å®Œæˆ
            state["current_step_completed"] = True

            # é€’å¢æ­¥éª¤ç´¢å¼•
            state["current_step_index"] = current_step_index + 1
            print(f"âœ… é€šç”¨æ™ºèƒ½ä½“æ­¥éª¤ç´¢å¼•å·²é€’å¢: {current_step_index} â†’ {current_step_index + 1}")

            # ä¿å­˜æ‰§è¡Œç»“æœä¾›åç»­æ­¥éª¤ä½¿ç”¨
            if "execution_results" not in state:
                state["execution_results"] = {}

            # ä½¿ç”¨ç‰¹æ®Šçš„keyæ¥æ ‡è¯†é€šç”¨æ™ºèƒ½ä½“çš„ç»“æœ
            general_agent_key = f"general_agent_step_{current_step_index + 1}"
            state["execution_results"][general_agent_key] = {
                "agent_name": "é€šç”¨æ™ºèƒ½ä½“",
                "result": response.content,
                "timestamp": datetime.datetime.now().isoformat(),
                "success": True,
                "step_id": current_step_index + 1
            }

            print(f"ğŸ’¾ é€šç”¨æ™ºèƒ½ä½“æ‰§è¡Œç»“æœå·²ä¿å­˜ï¼Œkey: {general_agent_key}")
        else:
            print(f"ğŸ“‹ é€šç”¨æ™ºèƒ½ä½“æ‰§è¡Œå•æ­¥ä»»åŠ¡æˆ–éå·¥ä½œæµä»»åŠ¡")

            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå•æ­¥ä»»åŠ¡å®Œæˆåä¹Ÿè¦é€’å¢æ­¥éª¤ç´¢å¼•ï¼Œè®©supervisorèƒ½æ£€æµ‹åˆ°å®Œæˆ
            state["current_step_completed"] = True
            state["current_step_index"] = current_step_index + 1
            print(f"âœ… å•æ­¥ä»»åŠ¡å®Œæˆï¼Œæ­¥éª¤ç´¢å¼•å·²é€’å¢: {current_step_index} â†’ {current_step_index + 1}")

            # ä¿å­˜æ‰§è¡Œç»“æœ
            if "execution_results" not in state:
                state["execution_results"] = {}

            # ä½¿ç”¨ç‰¹æ®Šçš„keyæ¥æ ‡è¯†é€šç”¨æ™ºèƒ½ä½“å•æ­¥ä»»åŠ¡çš„ç»“æœ
            single_step_key = f"general_agent_single_step"
            state["execution_results"][single_step_key] = {
                "agent_name": "é€šç”¨æ™ºèƒ½ä½“",
                "result": response.content,
                "timestamp": datetime.datetime.now().isoformat(),
                "success": True,
                "step_id": current_step_index + 1
            }

            print(f"ğŸ’¾ å•æ­¥ä»»åŠ¡æ‰§è¡Œç»“æœå·²ä¿å­˜ï¼Œkey: {single_step_key}")

    except Exception as e:
        # å¢å¼ºçš„å¼‚å¸¸å¤„ç†
        error_type = type(e).__name__
        error_msg = str(e)

        print(f"\n=== LLM è°ƒç”¨å¼‚å¸¸è¯¦æƒ… (JSONæ ¼å¼é”™è¯¯ä¸“é¡¹å¤„ç†) ===")
        print(f"å¼‚å¸¸ç±»å‹: {error_type}")
        print(f"å¼‚å¸¸æ¶ˆæ¯: {error_msg}")

        # ç‰¹æ®Šå¤„ç†JSONç›¸å…³é”™è¯¯
        if any(keyword in error_msg.lower() for keyword in ["expecting", "delimiter", "json", "invalid character"]):
            print(get_error_msg(type="json"))

            # å°è¯•é‡æ–°æ„å»ºç®€åŒ–çš„æ¶ˆæ¯
            try:
                print("\nğŸ”§ å°è¯•ä½¿ç”¨ç®€åŒ–æ¶ˆæ¯é‡æ–°è°ƒç”¨...")

                # åˆ›å»ºæœ€ç®€åŒ–çš„æ¶ˆæ¯ï¼ŒåªåŒ…å«æ ¸å¿ƒå†…å®¹
                simple_user_query = re.sub(r'[^\w\s\u4e00-\u9fff.,!?]', ' ', user_query) if user_query else "è¯·å¸®åŠ©æˆ‘"
                simple_user_query = re.sub(r'\s+', ' ', simple_user_query).strip()

                simple_messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·å¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚"),
                    HumanMessage(content=simple_user_query)
                ]

                print(f"ç®€åŒ–æŸ¥è¯¢: {simple_user_query}")

                model_name = state.get("model")

                response = await safe_llm_invoke(llm, config, model_name, simple_messages)

                # å¤„ç†å“åº”å†…å®¹
                if any(indicator in response.content.lower() for indicator in ['.md', 'markdown', '```']):
                    response.content = fix_markdown_display(response.content)

                new_message = AIMessage(content=response.content, name=cur_node)
                state["messages"].append(new_message)
                state["inner_messages"].append(new_message)

                print("âœ… ç®€åŒ–æ¶ˆæ¯è°ƒç”¨æˆåŠŸ")
                return state

            except Exception as retry_e:
                print(f"âŒ ç®€åŒ–æ¶ˆæ¯è°ƒç”¨ä¹Ÿå¤±è´¥: {str(retry_e)}")

                # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨é¢„è®¾å›å¤
                print("ğŸ”§ ä½¿ç”¨é¢„è®¾å›å¤ä½œä¸ºæœ€åçš„é™çº§æ–¹æ¡ˆ")
                fallback_content = f"æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†æŠ€æœ¯é—®é¢˜ã€‚æ‚¨çš„è¯·æ±‚æ˜¯ï¼š{simple_user_query if 'simple_user_query' in locals() else user_query}ã€‚è¯·å°è¯•ç”¨æ›´ç®€å•çš„æ–¹å¼é‡æ–°æé—®ã€‚"
                fallback_response = AIMessage(content=fallback_content, name=cur_node)
                state["messages"].append(fallback_response)
                state["inner_messages"].append(fallback_response)
                state['completed'] = True
                return state

        # å…¶ä»–ç±»å‹çš„é”™è¯¯å¤„ç†
        if "Connection" in error_type or "connection" in error_msg.lower():
            print(get_error_msg(type="connection"))

        elif "timeout" in error_msg.lower():
            print(get_error_msg(type="timeout"))

        elif "authentication" in error_msg.lower() or "unauthorized" in error_msg.lower():
            print(get_error_msg(type="authentication"))

        elif "rate limit" in error_msg.lower():
            print(get_error_msg(type="rate limit"))

        else:
            print(get_error_msg(type="other"))

        # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        import traceback
        print("\nå®Œæ•´é”™è¯¯å †æ ˆ:")
        traceback.print_exc()

        # è®°å½•ç¯å¢ƒä¿¡æ¯
        print("\n=== ç¯å¢ƒä¿¡æ¯ ===")
        print(f"Python ç‰ˆæœ¬: {sys.version}")
        print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")

        # è®¾ç½®é”™è¯¯å“åº”
        error_content = f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼ˆ{error_type}ï¼‰ã€‚è¯·æ‚¨ç¨åå†è¯•ã€‚å¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·å°è¯•ç®€åŒ–æ‚¨çš„è¯·æ±‚ã€‚"
        print(f"Error in agent_node: {error_msg}")
        state["messages"].append(AIMessage(content=error_content, name=cur_node))
        state["inner_messages"].append(AIMessage(content=error_content, name=cur_node))
        # è®¾ç½®å®Œæˆæ ‡å¿—é¿å…æ— é™å¾ªç¯
        state['completed'] = True

    return {
        "last_node": cur_node,
        "messages": state['messages'],
        "inner_messages": state['inner_messages'],
        "temporary_message_content_list": state["temporary_message_content_list"],
        "completed": state['completed'],
        "current_step_completed": state["current_step_completed"],
        "current_step_index": state["current_step_index"],
        "execution_results": state["execution_results"],
        "logs":state["logs"],
        "e2b_sandbox_id": state["e2b_sandbox_id"],
        "iteration_count": state["iteration_count"],
        "max_iterations": state["max_iterations"],
        "temporary_images": state["temporary_images"],
        "structure_tool_results": state["structure_tool_results"],
        "mcp_tools": state["mcp_tools"],
        "input_data": state["input_data"],
        "model": state.get("model", global_config.BASE_LLM),
        "a2a_agents": state["a2a_agents"],
        "a2a_sessions": state["a2a_sessions"],
        "route_to_a2a": state.get("route_to_a2a", None),
        "last_a2a_result": state.get("last_a2a_result", None),
        "a2a_failure_count": state["a2a_failure_count"],
        "a2a_fallback_to_general": state["a2a_fallback_to_general"],
        "failed_a2a_agents": state["failed_a2a_agents"],
        "supervisor_retry_count": state["supervisor_retry_count"],
        "supervisor_decision": state["supervisor_decision"],
        "workflow_steps": state["workflow_steps"],
        "workflow_plan": state.get("workflow_plan", None)
    }


async def process_a2a_agents(state: AgentState) -> AgentState:

    # ä¿å­˜åŸå§‹æ•°æ®
    original_a2a_agents = state.get("a2a_agents", [])  # ä¿å­˜åŸå§‹A2Aæ™ºèƒ½ä½“æ•°æ®

    # ä»inputå­—æ®µè·å–A2Aæ™ºèƒ½ä½“æ•°æ®
    if not original_a2a_agents:
        input_data = state.get("input", {})
        if input_data and "a2a_agents" in input_data:
            original_a2a_agents = input_data["a2a_agents"]
            print(f"[initial_setup_node] ä» input ä¸­è·å– a2a_agents: {len(original_a2a_agents)} ä¸ªæ™ºèƒ½ä½“")

    # è®¾ç½®A2Aæ™ºèƒ½ä½“æ•°æ®
    if original_a2a_agents:
        state["a2a_agents"] = original_a2a_agents
        print(f"[initial_setup_node] è®¾ç½® a2a_agents: {len(original_a2a_agents)} ä¸ªæ™ºèƒ½ä½“")

        # å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯ä¸ªA2Aæ™ºèƒ½ä½“çš„è¯¦ç»†ä¿¡æ¯
        print("\n========== A2A æ™ºèƒ½ä½“è¯¦ç»†ä¿¡æ¯ ==========")
        for idx, agent in enumerate(original_a2a_agents, 1):
            print(f"\n--- A2Aæ™ºèƒ½ä½“ {idx} ---")
            print(f"  ID: {agent.get('agent_id', agent.get('agent_Id', 'Unknown'))}")
            print(f"  åç§°: {agent.get('name', 'Unknown')}")
            print(f"  æè¿°: {agent.get('desc', agent.get('description', 'No description'))}")
        print("=====================================\n")

    # æ¶ˆæ¯å¤„ç†å·²ç”± create_initial_state ç»Ÿä¸€å¤„ç†ï¼Œæ— éœ€é‡å¤æ·»åŠ 
    print(f"[initial_setup_node] æ¶ˆæ¯å·²ç”± create_initial_state å¤„ç†ï¼Œå½“å‰æ¶ˆæ¯æ•°é‡: {len(state.get('messages', []))}")

    return state


async def process_mcp_tools(state: AgentState) -> AgentState:

    # ä¿å­˜åŸå§‹æ•°æ®
    original_mcp_tools = state.get("mcp_tools", [])

    # ä»inputå­—æ®µè·å–æ•°æ®
    if not original_mcp_tools:
        input_data = state.get("input", {})
        if input_data and "mcp_tools" in input_data:
            original_mcp_tools = input_data["mcp_tools"]
            print(f"[initial_setup_node] ä» input ä¸­è·å– mcp_tools: {len(original_mcp_tools)} ä¸ªå·¥å…·")

    # è®¾ç½®MCPå·¥å…·æ•°æ®
    if original_mcp_tools:
        normalized_mcp_tools = []
        for tool in original_mcp_tools:
            normalized_tool = normalize_mcp_tool_data(tool)
            normalized_mcp_tools.append(normalized_tool)

        state["mcp_tools"] = normalized_mcp_tools
        print(f"[initial_setup_node] è®¾ç½®æ ‡å‡†åŒ–åçš„ mcp_tools: {len(normalized_mcp_tools)} ä¸ªå·¥å…·")

        # å¢å¼ºè°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ¯ä¸ªå·¥å…·çš„è¯¦ç»†ä¿¡æ¯
        print("\n========== MCP å·¥å…·è¯¦ç»†ä¿¡æ¯ ==========")
        for idx, tool in enumerate(normalized_mcp_tools, 1):
            print(f"\n--- å·¥å…· {idx} ---")
            print(f"  åç§°: {tool.get('name', 'Unknown')}")
            print(f"  ID: {tool.get('id', tool.get('tool_id', 'Unknown'))}")
            print(f"  ç±»å‹: {tool.get('type', 'unknown')}")
            print(f"  æè¿°: {tool.get('description', tool.get('desc', 'No description'))}")

            # æ‰“å°å‚æ•°ä¿¡æ¯
            params = tool.get('parameters', tool.get('arguments', {}))
            if params:
                print(f"  å‚æ•°ç»“æ„:")
                print(f"    {json.dumps(params, indent=4, ensure_ascii=False)}")
            else:
                print(f"  å‚æ•°ç»“æ„: æ— ")

            # å¦‚æœæœ‰é¢„å®šä¹‰å‚æ•°å€¼
            if 'arguments' in tool and isinstance(tool['arguments'], list):
                print(f"  é¢„å®šä¹‰å‚æ•°å€¼:")
                for arg in tool['arguments']:
                    print(f"    {json.dumps(arg, ensure_ascii=False)}")
        print("=====================================\n")

    return state


async def process_knowledge(state: AgentState, config: RunnableConfig) -> AgentState:
    # todo åˆå§‹åŒ– çŸ¥è¯†åº“ä¿¡æ¯

    return state



# ä½¿ç”¨ MarkItDown è½¬æ¢å¹¶å†™å…¥ä¸Šä¸‹æ–‡
async def process_attachment(state: AgentState, config: RunnableConfig) -> AgentState:
    """
    é™„ä»¶å¤„ç†èŠ‚ç‚¹
    - ä» state["files"] è¯»å–æœ¬åœ°é™„ä»¶è·¯å¾„
    - ä½¿ç”¨ convert_to_markdown_unified è½¬æ¢å¹¶å†™å…¥ E2B æ²™ç®±
    - ä»æ²™ç®±è¯»å–ç”Ÿæˆçš„ .md å†…å®¹ï¼Œæ±‡æ€»åˆ° state["inner_messages"]
    """
    print("=== Attachment Processor Node (MarkItDown) å¼€å§‹ ===")
    # state["files"] å­˜æœ¬åœ°é™„ä»¶è·¯å¾„
    files = state.get("files", [])
    if not files:
        print("[attachment_processor] æœªå‘ç°é™„ä»¶æ–‡ä»¶ï¼Œè·³è¿‡å¤„ç†")
        return state
        
    if files[0].strip().split(".")[-1].lower() in ["jpg", "jpeg", "png", "gif", "bmp", "webp"]:
        context_content = "å›¾ç‰‡åœ°å€ä¸ºï¼š" + files[0]
        state["inner_messages"].append(AIMessage(content=context_content, name="attachment"))
        return state

    # æ—¥å¿—ï¼šå¼€å§‹å¤„ç†
    if "logs" not in state:
        state["logs"] = []
    state["logs"].append({
        "message": f"å‘ç°{len(files)}ä¸ªé™„ä»¶ï¼Œæ­£åœ¨è½¬æ¢ä¸ºMarkdown...",
        "done": False,
        "messageId": get_last_show_message_id(state.get("messages", []))
    })

    context_parts = ["=== é™„ä»¶å†…å®¹è§£æç»“æœ ===\n"]

    for idx, file in enumerate(files, 1):
        try:
            if not isinstance(file, str):
                print(f"éå­—ç¬¦ä¸²é™„ä»¶è·¯å¾„ï¼Œè·³è¿‡: {type(file)}")
                continue
            src_path = file
            file_name = os.path.basename(src_path)

            if not os.path.exists(src_path):
                print(f"æœ¬åœ°é™„ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {src_path}")
                continue

            print(f"ä½¿ç”¨æœ¬åœ°é™„ä»¶è·¯å¾„: {src_path}")

            # åœ¨æ²™ç®±ä¸­ä¿å­˜çš„ Markdown æ–‡ä»¶è·¯å¾„
            stem = os.path.splitext(file_name)[0]
            sandbox_md_path = f"/home/md/attachment_{idx}_{stem}.md"

            # ç›´æ¥ä» state è¯»å– e2b_sandbox_id
            sandbox_id = state.get("e2b_sandbox_id")
            if not sandbox_id:
                print("æ²™ç®±æœªå®ä¾‹åŒ–ï¼Œæ— æ³•è¿›è¡Œæ²™ç®±è½¬æ¢ï¼Œè·³è¿‡è¯¥é™„ä»¶ã€‚")
                continue

            # å…ˆè¿æ¥ä¸€æ¬¡æ²™ç®±ï¼Œåç»­å†™å…¥ä¸è¯»å–å¤ç”¨åŒä¸€è¿æ¥
            from e2b import Sandbox
            sbx = await asyncio.to_thread(Sandbox.connect, str(sandbox_id))

            # æ‰§è¡Œç»Ÿä¸€çš„æ²™ç®± Markdown è½¬æ¢
            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­å°†æ½œåœ¨é˜»å¡çš„æ–‡ä»¶è§£æä¸å†™å…¥æ²™ç®±æ“ä½œå¸è½½åˆ°çº¿ç¨‹
            result = await asyncio.to_thread(
                convert_to_markdown_unified,
                file_path=src_path,
                sandbox_id=str(sandbox_id),
                sandbox_path=sandbox_md_path,
                output_path=None,
                sbx=sbx,
            )
            file_type = result.get("file_type", "unknown")
            sandbox_path = result.get("sandbox_path", sandbox_md_path)

            print(f"è½¬æ¢å®Œæˆ: ç±»å‹={file_type}, æ²™ç®±Markdown={sandbox_path}")

            # ä»æ²™ç®±è¯»å–ç”Ÿæˆçš„ Markdown å†…å®¹ï¼Œå¢åŠ çŸ­æš‚é‡è¯•ï¼Œé¿å…å†™å…¥åç«‹å³è¯»å–å¯¼è‡´ç©º
            md_content = ""
            try:
                last_err = None
                for attempt in range(5):
                    try:
                        content = await asyncio.to_thread(sbx.files.read, sandbox_path)
                        if isinstance(content, bytes):
                            content = content.decode("utf-8", errors="replace")
                        if isinstance(content, str) and content.strip():
                            md_content = content
                            break
                        await asyncio.sleep(0.4 * (2 ** attempt))
                    except Exception as e:
                        last_err = e
                        await asyncio.sleep(0.4 * (2 ** attempt))
                if not md_content:
                    raise RuntimeError(f"æ²™ç®±è¯»å–å†…å®¹ä¸ºç©ºæˆ–å¤±è´¥ï¼Œpath={sandbox_path}, err={last_err}")
            except Exception as read_err:
                md_content = f"è¯»å–æ²™ç®±Markdownå¤±è´¥: {str(read_err)}"

            # æ±‡æ€»ä¸Šä¸‹æ–‡ï¼ˆå¯¹å†…å®¹è¿›è¡Œé•¿åº¦æ§åˆ¶ï¼‰
            header = f"**é™„ä»¶ {file_name}** (ç±»å‹: {file_type}):\n"
            context_parts.append(header)
            max_chars = int(os.getenv("ATTACHMENT_CONTEXT_MAX_CHARS", "120000"))
            # max_chars = 120000
            current_len = sum(len(p) for p in context_parts)
            remaining = max_chars - current_len
            context_parts.append(md_content[:remaining] if remaining > 0 else "")
            context_parts.append("\n")

        except Exception as e:
            print(f"âŒ å¤„ç†é™„ä»¶å¤±è´¥: {str(e)}")
            # å¯¹äºå­—ç¬¦ä¸²è·¯å¾„ï¼Œæå–æ–‡ä»¶å
            file_name = os.path.basename(file) if isinstance(file, str) else f"attachment_{idx}"
            context_parts.append(f"**é™„ä»¶ {file_name}** å¤„ç†å¤±è´¥: {str(e)}\n")

    # ç»„åˆä¸Šä¸‹æ–‡å†…å®¹
    context_parts.append("=== åŸºäºä»¥ä¸Šé™„ä»¶å†…å®¹ï¼Œè¯·å›ç­”ç”¨æˆ·é—®é¢˜ ===\n")
    context_content = "\n".join(context_parts)

    # å†™å…¥åˆ°ä¸Šä¸‹æ–‡æ¶ˆæ¯
    state["inner_messages"].append(AIMessage(content=context_content, name="attachment"))

    # å®Œæˆæ—¥å¿—
    state["logs"].append({
        "message": "é™„ä»¶è§£æå®Œæˆï¼Œå·²ç”ŸæˆMarkdownå¹¶å†™å…¥ä¸Šä¸‹æ–‡",
        "done": True,
        "messageId": get_last_show_message_id(state.get("messages", []))
    })

    print("=== Attachment Processor Node (MarkItDown) å®Œæˆ ===")
    return state


def route_after_agent_node(last_message):
    if last_message and hasattr(last_message, "tool_calls") and last_message.tool_calls:
        print(f"[é€šç”¨æ™ºèƒ½ä½“è·¯ç”±] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {[tool_call['name'] for tool_call in last_message.tool_calls]}")
        return "tool_executor"
    else:
        return "supervisor"

