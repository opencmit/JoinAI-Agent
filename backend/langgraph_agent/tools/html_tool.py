import os
import re


from langchain_openai import ChatOpenAI
from langchain.tools.base import BaseTool
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from langgraph_agent.graph.state import AgentState
from langgraph_agent.utils.tool_utils import fix_markdown_display, process_raw_html_content




async def html_generation_node(
        state: AgentState,
        config: RunnableConfig,
) -> AgentState:
    """
    å¤„ç†HTMLç”Ÿæˆè¯·æ±‚ï¼Œç”ŸæˆHTMLæ–‡ä»¶å¹¶ä¿å­˜ - é›†æˆå†…å®¹æå–åŠŸèƒ½çš„ç‰ˆæœ¬
    """
    print("=== HTML Generation Node å¼€å§‹ ===")

    # ğŸ”¥ æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼Œé¿å…é‡å¤å¤„ç†
    if state.get("html_generation_processed", False):
        print("HTMLç”Ÿæˆå·²å¤„ç†ï¼Œè·³è¿‡")
        return state

    # è·å–ç”¨æˆ·çš„å®Œæ•´æŸ¥è¯¢
    user_query = ""
    for message in reversed(state.get("messages", [])):
        if hasattr(message, 'type') and message.type == "human":
            user_query = message.content
            break

    print(f"ç”¨æˆ·æŸ¥è¯¢: {user_query}")

    # ä½¿ç”¨LLMå®¢æˆ·ç«¯
    model_name = os.getenv('HTML_BASE_LLM', 'DeepSeek-R1')
    llm_client = ChatOpenAI(
        api_key=os.getenv('HTML_OPENAI_API_KEY', 'sk-ef6a231232834ee7ab363c1269d2eb4e'),
        base_url=os.getenv('OPENAI_BASE_URL', 'https://api.deepseek.com'),
        model=model_name,
        temperature=0.1,
    )

    try:
        # ç›´æ¥ä½¿ç”¨ç”¨æˆ·åŸå§‹æŸ¥è¯¢ç”ŸæˆHTML
        html_messages = [
            HumanMessage(content=user_query)
        ]

        html_response = await safe_llm_invoke(llm_client, config, model_name, html_messages)
        raw_content = html_response.content

        print(f"LLMç”Ÿæˆçš„åŸå§‹å†…å®¹é•¿åº¦: {len(raw_content)} å­—ç¬¦")

        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨å†…å®¹æå–å‡½æ•°å¤„ç†åŸå§‹å†…å®¹
        response_content, filename = await process_raw_html_content(
            raw_content, state, config, user_query
        )

        # æ·»åŠ å“åº”æ¶ˆæ¯
        state["messages"].append(AIMessage(content=response_content))

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ ‡è®°å·²å¤„ç†
        state["html_generation_processed"] = True
        state["force_attachment_call"] = False
        state["completed"] = True

    except Exception as e:
        print(f"HTMLç”Ÿæˆå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        error_response = f"""âŒ **HTMLç”Ÿæˆå¤„ç†å¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯·æ£€æŸ¥æ‚¨çš„è¯·æ±‚å¹¶é‡è¯•ã€‚æ‚¨å¯ä»¥å°è¯•ï¼š
1. æ›´å…·ä½“åœ°æè¿°æ‚¨éœ€è¦çš„HTMLé¡µé¢
2. è¯´æ˜é¡µé¢çš„ä¸»è¦åŠŸèƒ½å’Œå†…å®¹
3. æä¾›è®¾è®¡å‚è€ƒæˆ–é£æ ¼è¦æ±‚"""

        state["messages"].append(AIMessage(content=error_response))
        state["html_generation_processed"] = True

    print("=== HTML Generation Node å®Œæˆ ===")
    return state


async def markdown_to_html_node(
        state: AgentState,
        config: RunnableConfig,
) -> AgentState:
    """
    å¤„ç†markdownç”Ÿæˆè¯·æ±‚ï¼Œç”ŸæˆHTMLæ–‡ä»¶å¹¶ä¿å­˜

    Args:
        state: å½“å‰çŠ¶æ€
        config: è¿è¡Œé…ç½®

    Returns:
        AgentState: æ›´æ–°åçš„çŠ¶æ€
    """
    print("=== Markdown to HTML Node å¼€å§‹ ===")

    # è·å–ç”¨æˆ·çš„å®Œæ•´æŸ¥è¯¢
    user_query = ""
    for message in reversed(state.get("messages", [])):
        if hasattr(message, 'type') and message.type == "human":
            user_query = message.content
            break

    print(f"ç”¨æˆ·æŸ¥è¯¢: {user_query}")

    model_name = state.get("model")

    # ä½¿ç”¨LLMå®¢æˆ·ç«¯
    from langgraph_agent.graph.graph import AgentGraph
    agent_graph = AgentGraph()
    llm_client, _ = agent_graph._get_llm_client(state, config)

    try:
        # ç¬¬ä¸€æ­¥ï¼šè§£æç”¨æˆ·éœ€æ±‚ï¼Œæå–markdownå†…å®¹
        parse_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚ï¼Œæå–å…¶ä¸­å…³äºmarkdownæ–‡ä»¶çš„éœ€æ±‚ã€‚

ç”¨æˆ·è¯·æ±‚ï¼š{user_query}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯ï¼š
1. markdownæ–‡ä»¶çš„ä¸»é¢˜æˆ–æ ‡é¢˜
2. markdownåº”åŒ…å«çš„ä¸»è¦å†…å®¹è¦ç‚¹
3. ä»»ä½•ç‰¹å®šçš„æ ¼å¼è¦æ±‚

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œä¾‹å¦‚ï¼š
{{
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "content_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
    "format_requirements": "æ ¼å¼è¦æ±‚æè¿°"
}}"""

        parse_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç†è§£ç”¨æˆ·éœ€æ±‚å¹¶æå–å…³é”®ä¿¡æ¯ã€‚"),
            HumanMessage(content=parse_prompt)
        ]

        parse_response = await safe_llm_invoke(llm_client, config, model_name, parse_messages)

        # å°è¯•è§£æJSONå“åº”
        import json
        import re

        # æå–JSONå†…å®¹
        json_match = re.search(r'\{[\s\S]*\}', parse_response.content)
        if json_match:
            try:
                parsed_info = json.loads(json_match.group())
            except:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                parsed_info = {
                    "title": "Markdownæ–‡æ¡£",
                    "content_points": [user_query],
                    "format_requirements": "æ ‡å‡†markdownæ ¼å¼"
                }
        else:
            parsed_info = {
                "title": "Markdownæ–‡æ¡£",
                "content_points": [user_query],
                "format_requirements": "æ ‡å‡†markdownæ ¼å¼"
            }

        print(f"è§£æçš„ä¿¡æ¯: {parsed_info}")

        # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆmarkdownå†…å®¹
        markdown_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„markdownæ–‡æ¡£ï¼š

æ ‡é¢˜ï¼š{parsed_info.get('title', 'Markdownæ–‡æ¡£')}
å†…å®¹è¦ç‚¹ï¼š{', '.join(parsed_info.get('content_points', [user_query]))}
æ ¼å¼è¦æ±‚ï¼š{parsed_info.get('format_requirements', 'æ ‡å‡†markdownæ ¼å¼')}

è¯·ç”Ÿæˆä¸€ä¸ªç»“æ„æ¸…æ™°ã€å†…å®¹ä¸°å¯Œçš„markdownæ–‡æ¡£ã€‚åŒ…æ‹¬é€‚å½“çš„æ ‡é¢˜å±‚çº§ã€åˆ—è¡¨ã€ä»£ç å—ï¼ˆå¦‚æœéœ€è¦ï¼‰ç­‰markdownå…ƒç´ ã€‚

ç›´æ¥è¿”å›markdownå†…å®¹ï¼Œä¸è¦åŒ…å«é¢å¤–çš„è¯´æ˜ã€‚"""

        markdown_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£ç¼–å†™åŠ©æ‰‹ï¼Œæ“…é•¿åˆ›å»ºç»“æ„åŒ–çš„markdownæ–‡æ¡£ã€‚"),
            HumanMessage(content=markdown_prompt)
        ]

        markdown_response = await safe_llm_invoke(llm_client, config, model_name, markdown_messages)
        original_markdown_content = markdown_response.content

        print(f"ç”Ÿæˆçš„Markdowné•¿åº¦: {len(original_markdown_content)} å­—ç¬¦")

        # ç¬¬ä¸‰æ­¥ï¼šå°†markdownè½¬æ¢ä¸ºå®Œæ•´çš„HTMLæ–‡ä»¶
        html_prompt = f"""è¯·å°†ä»¥ä¸‹markdownå†…å®¹è½¬æ¢ä¸ºä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥è¿è¡Œçš„HTMLæ–‡ä»¶ã€‚

Markdownå†…å®¹ï¼š
{original_markdown_content}

è¦æ±‚ï¼š
1. ç”Ÿæˆå®Œæ•´çš„HTML5æ–‡æ¡£ç»“æ„ï¼ˆåŒ…å«<!DOCTYPE html>, <html>, <head>, <body>ç­‰ï¼‰
2. åœ¨<head>ä¸­åŒ…å«é€‚å½“çš„metaæ ‡ç­¾ï¼ˆcharset, viewportç­‰ï¼‰
3. æ·»åŠ ç¾è§‚çš„CSSæ ·å¼ï¼ŒåŒ…æ‹¬ï¼š
   - å“åº”å¼è®¾è®¡
   - ä»£ç é«˜äº®æ ·å¼
   - è¡¨æ ¼æ ·å¼
   - å¼•ç”¨å—æ ·å¼
   - åˆ—è¡¨æ ·å¼
4. å¦‚æœæœ‰ä»£ç å—ï¼Œæ·»åŠ è¯­æ³•é«˜äº®æ”¯æŒ
5. ä½¿ç”¨ç°ä»£ã€æ¸…æ™°çš„æ’ç‰ˆé£æ ¼
6. æ·»åŠ é€‚å½“çš„å†…è¾¹è·å’Œå¤–è¾¹è·
7. ä½¿ç”¨æ˜“è¯»çš„å­—ä½“å’Œé¢œè‰²æ–¹æ¡ˆ

ç›´æ¥è¿”å›å®Œæ•´çš„HTMLä»£ç ï¼Œä¸è¦åŒ…å«markdownä»£ç å—æ ‡è®°ã€‚"""

        html_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‰ç«¯å¼€å‘è€…ï¼Œæ“…é•¿å°†markdownè½¬æ¢ä¸ºç¾è§‚çš„HTMLé¡µé¢ã€‚"),
            HumanMessage(content=html_prompt)
        ]

        html_response = await safe_llm_invoke(llm_client, config, model_name, html_messages)
        html_content = html_response.content

        # æ¸…ç†HTMLå†…å®¹ï¼ˆç§»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°ï¼‰
        html_content = html_content.strip()
        if html_content.startswith("```html"):
            html_content = html_content[7:]
        if html_content.endswith("```"):
            html_content = html_content[:-3]
        html_content = html_content.strip()

        print(f"ç”Ÿæˆçš„HTMLé•¿åº¦: {len(html_content)} å­—ç¬¦")

        # ç¬¬å››æ­¥ï¼šä½¿ç”¨æ–‡ä»¶å·¥å…·ä¿å­˜HTMLæ–‡ä»¶
        # ç”Ÿæˆæ–‡ä»¶å
        import time
        timestamp = int(time.time())
        safe_title = parsed_info.get('title', 'document').replace(' ', '_').replace('/', '_')[:50]
        filename = f"{safe_title}_{timestamp}.html"

        # åˆ¤æ–­æ˜¯å¦æœ‰æ²™ç®±ç¯å¢ƒ
        try:
            # è°ƒç”¨æ–‡ä»¶å·¥å…·ä¿å­˜æ–‡ä»¶
            from langgraph_agent.tools import files_tool

            # å‡†å¤‡æ–‡ä»¶å·¥å…·çš„å‚æ•°
            tool_args = {
                "operation": "create",
                "path": filename,
                "content": html_content,
                "state": state,
                "special_config_param": {"encoding": "utf-8"}
            }

            # æ‰§è¡Œæ–‡ä»¶ä¿å­˜
            print(f"æ­£åœ¨ä¿å­˜HTMLæ–‡ä»¶: {filename}")
            result_state, save_result = await files_tool.ainvoke(tool_args, config=config)

            # æ›´æ–°çŠ¶æ€
            state = result_state

            # æ„å»ºå“åº”æ¶ˆæ¯
            if "æˆåŠŸ" in str(save_result) or "created" in str(save_result).lower():
                # ä¸ºæ˜¾ç¤ºå‡†å¤‡å†…å®¹
                display_markdown = fix_markdown_display(original_markdown_content)
                response_content = f"""âœ… **Markdownè½¬HTMLä»»åŠ¡å®Œæˆ**

**åŸå§‹éœ€æ±‚**: {user_query}

**ç”Ÿæˆçš„æ–‡æ¡£ä¿¡æ¯**:
- æ ‡é¢˜: {parsed_info.get('title', 'Markdownæ–‡æ¡£')}
- Markdowné•¿åº¦: {len(original_markdown_content)} å­—ç¬¦
- HTMLæ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦

**æ–‡ä»¶å·²ä¿å­˜**:
- æ–‡ä»¶å: `{filename}`
- ä½ç½®: æ–‡ä»¶ç³»ç»Ÿæ ¹ç›®å½•

**Markdownå†…å®¹é¢„è§ˆ**:
```markdown
{display_markdown[:500]}...
```

**HTMLå†…å®¹é¢„è§ˆ** (å‰500å­—ç¬¦):
```html
{html_content[:500]}...
```

æ‚¨å¯ä»¥é€šè¿‡æ–‡ä»¶å·¥å…·æŸ¥çœ‹æˆ–ä¸‹è½½å®Œæ•´çš„HTMLæ–‡ä»¶ã€‚"""
            else:
                # æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œæä¾›å¤‡ç”¨æ–¹æ¡ˆ
                display_markdown = fix_markdown_display(original_markdown_content)
                response_content = f"""âš ï¸ **Markdownè½¬HTMLä»»åŠ¡å®Œæˆï¼ˆæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼‰**

ç”±äºæ²™ç®±ç¯å¢ƒé—®é¢˜ï¼Œæ–‡ä»¶ä¿å­˜å¤±è´¥ã€‚ä½†HTMLå†…å®¹å·²æˆåŠŸç”Ÿæˆã€‚

**ç”Ÿæˆçš„æ–‡æ¡£ä¿¡æ¯**:
- æ ‡é¢˜: {parsed_info.get('title', 'Markdownæ–‡æ¡£')}
- Markdowné•¿åº¦: {len(original_markdown_content)} å­—ç¬¦
- HTMLæ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦

**å®Œæ•´çš„Markdownå†…å®¹**:
```markdown
{display_markdown}
```

**å®Œæ•´çš„HTMLå†…å®¹**:
```html
{html_content}
```

æ‚¨å¯ä»¥ï¼š
1. å¤åˆ¶ä¸Šè¿°HTMLå†…å®¹
2. æ‰‹åŠ¨ä¿å­˜ä¸º `{filename}`
3. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŸ¥çœ‹æ•ˆæœ"""

        except Exception as file_error:
            print(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(file_error)}")

            # å¦‚æœæ–‡ä»¶å·¥å…·å¤±è´¥ï¼Œè¿”å›å®Œæ•´å†…å®¹
            display_markdown = fix_markdown_display(original_markdown_content)
            response_content = f"""âš ï¸ **Markdownè½¬HTMLä»»åŠ¡å®Œæˆï¼ˆä¿å­˜å¤±è´¥ï¼‰**

HTMLå†…å®¹å·²ç”Ÿæˆï¼Œä½†æ— æ³•ä¿å­˜æ–‡ä»¶ï¼š{str(file_error)}

**ç”Ÿæˆçš„Markdownå†…å®¹**:
```markdown
{display_markdown}
```

**ç”Ÿæˆçš„HTMLå†…å®¹**:
```html
{html_content}
```

è¯·æ‰‹åŠ¨å¤åˆ¶å¹¶ä¿å­˜ä¸º `{filename}`ã€‚"""

        # æ·»åŠ å“åº”æ¶ˆæ¯
        state["messages"].append(AIMessage(content=response_content))

        # æ ‡è®°markdownè¯·æ±‚å·²å¤„ç†
        state["markdown_processed"] = True
        state["force_attachment_call"] = False  # ç¡®ä¿ä¸ä¼šè§¦å‘attachmentå¤„ç†

    except Exception as e:
        print(f"Markdownå¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        error_response = f"""âŒ **Markdownè½¬HTMLå¤„ç†å¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯·æ£€æŸ¥æ‚¨çš„è¯·æ±‚å¹¶é‡è¯•ã€‚"""

        state["messages"].append(AIMessage(content=error_response))
        state["markdown_processed"] = True

    print("=== Markdown to HTML Node å®Œæˆ ===")
    return state


async def enhanced_markdown_to_html_node(
        state: AgentState,
        config: RunnableConfig,
) -> AgentState:
    """
    å¢å¼ºç‰ˆï¼šå¤„ç†åŒ…å«æ•°æ®åˆ†æçš„markdownç”Ÿæˆè¯·æ±‚ - é›†æˆHTMLæå–åŠŸèƒ½
    """
    print("=== Enhanced Markdown to HTML Node å¼€å§‹ ===")

    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if state.get("markdown_processed", False):
        print("Markdownå¤„ç†å·²å®Œæˆï¼Œè·³è¿‡")
        return state

    # æ£€æŸ¥HTMLæ˜¯å¦å·²ç»ç”Ÿæˆ
    html_already_generated = state.get("html_generation_processed", False)

    # è·å–ç”¨æˆ·çš„å®Œæ•´æŸ¥è¯¢
    user_query = ""
    for message in reversed(state.get("messages", [])):
        if hasattr(message, 'type') and message.type == "human":
            user_query = message.content
            break

    print(f"ç”¨æˆ·æŸ¥è¯¢é•¿åº¦: {len(user_query)} å­—ç¬¦")

    model_name = os.getenv('HTML_BASE_LLM', 'DeepSeek-R1')

    # ä½¿ç”¨LLMå®¢æˆ·ç«¯
    llm_client = ChatOpenAI(
        api_key=os.getenv('OPENAI_API_KEY', 'sk-ef6a231232834ee7ab363c1269d2eb4e'),
        base_url=os.getenv('OPENAI_BASE_URL', 'https://api.deepseek.com'),
        model=model_name,
        temperature=0.1,
    )

    try:
        # æ£€æµ‹æ˜¯å¦åŒ…å«æ•°æ®è¡¨æ ¼
        has_data_table = any(keyword in user_query for keyword in ['rpm', 'tpm', 'æ•°æ®', 'è¡¨æ ¼', 'ç»Ÿè®¡'])

        # ç”ŸæˆMarkdownå†…å®¹çš„é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if has_data_table:
            # æ•°æ®åˆ†æç›¸å…³çš„å¤„ç†...
            pass

        # ç”Ÿæˆmarkdown
        markdown_messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£å’Œæ•°æ®åˆ†ææŠ¥å‘Šç¼–å†™ä¸“å®¶ã€‚"),
            HumanMessage(content=user_query)
        ]

        markdown_response = await safe_llm_invoke(llm_client, config, model_name, markdown_messages)
        original_markdown_content = markdown_response.content

        print(f"ç”Ÿæˆçš„Markdowné•¿åº¦: {len(original_markdown_content)} å­—ç¬¦")

        # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šå¦‚æœéœ€è¦ç”ŸæˆHTMLï¼Œä½¿ç”¨æå–å‡½æ•°å¤„ç†
        html_response_content = ""
        if not html_already_generated:
            print("HTMLæœªç”Ÿæˆï¼Œå¼€å§‹ç”ŸæˆHTMLå†…å®¹")

            # ç”ŸæˆHTMLçš„æç¤ºè¯
            html_prompt = f"""å°†ä»¥ä¸‹markdownè½¬æ¢ä¸ºHTMLé¡µé¢ï¼š

{original_markdown_content}

è¯·ç”Ÿæˆå®Œæ•´å¯ç”¨çš„HTMLä»£ç ã€‚"""

            html_messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‰ç«¯å¼€å‘ä¸“å®¶ï¼Œæ“…é•¿åˆ›å»ºäº¤äº’å¼æ•°æ®å¯è§†åŒ–é¡µé¢ã€‚"),
                HumanMessage(content=html_prompt)
            ]

            html_response = await safe_llm_invoke(llm_client, config, model_name, html_messages)
            raw_html_content = html_response.content

            # ğŸ”¥ ä½¿ç”¨æå–å‡½æ•°å¤„ç†HTMLå†…å®¹
            html_response_content, html_filename = await process_raw_html_content(
                raw_html_content, state, config, f"markdown_to_html_{user_query[:20]}"
            )

        # ä¿å­˜Markdownæ–‡ä»¶
        try:
            from langgraph_agent.tools import files_tool

            # ç”ŸæˆMarkdownæ–‡ä»¶å
            import time
            timestamp = int(time.time())
            safe_title = re.sub(r'[^\w\u4e00-\u9fff]', '_', user_query[:30])
            md_filename = f"{safe_title}_{timestamp}.md"

            # ä¿å­˜Markdownæ–‡ä»¶
            md_tool_args = {
                "operation": "create",
                "path": md_filename,
                "content": original_markdown_content,
                "state": state,
                "special_config_param": {"encoding": "utf-8"}
            }

            result_state, md_save_result = await files_tool.ainvoke(md_tool_args, config=config)
            state = result_state

            # æ„å»ºæœ€ç»ˆå“åº”
            md_success = "æˆåŠŸ" in str(md_save_result) or "created" in str(md_save_result).lower()

            if md_success:
                if not html_already_generated and html_response_content:
                    # Markdownå’ŒHTMLéƒ½æˆåŠŸ
                    response_content = f"""âœ… **Markdownå’ŒHTMLæ–‡ä»¶ç”Ÿæˆå®Œæˆ**

**Markdownæ–‡ä»¶**: `{md_filename}` âœ…

{html_response_content}

**Markdownå†…å®¹é¢„è§ˆ**:
```markdown
{fix_markdown_display(original_markdown_content[:300])}...
```"""
                else:
                    # åªæœ‰MarkdownæˆåŠŸ
                    response_content = f"""âœ… **Markdownæ–‡æ¡£ç”Ÿæˆå®Œæˆ**

**Markdownæ–‡ä»¶**: `{md_filename}` âœ…
**HTMLæ–‡ä»¶**: å·²ç”±HTMLç”Ÿæˆå™¨å¤„ç†

**Markdownå†…å®¹é¢„è§ˆ**:
```markdown
{fix_markdown_display(original_markdown_content[:500])}...
```"""
            else:
                # Markdownä¿å­˜å¤±è´¥
                response_content = f"""âš ï¸ **Markdownä¿å­˜å¤±è´¥ä½†å†…å®¹å·²ç”Ÿæˆ**

**ä¿å­˜ç»“æœ**: {md_save_result}

{html_response_content if html_response_content else ""}

**ç”Ÿæˆçš„Markdownå†…å®¹**:
```markdown
{fix_markdown_display(original_markdown_content)}
```"""

        except Exception as file_error:
            print(f"Markdownæ–‡ä»¶ä¿å­˜å¤±è´¥: {str(file_error)}")
            response_content = f"""âŒ **Markdownæ–‡ä»¶ä¿å­˜å¼‚å¸¸**

**é”™è¯¯ä¿¡æ¯**: {str(file_error)}

{html_response_content if html_response_content else ""}

**ç”Ÿæˆçš„Markdownå†…å®¹**:
```markdown
{fix_markdown_display(original_markdown_content)}
```"""

        # æ·»åŠ å“åº”æ¶ˆæ¯
        state["messages"].append(AIMessage(content=response_content))

        # æ ‡è®°å·²å¤„ç†
        state["markdown_processed"] = True
        state["force_attachment_call"] = False

    except Exception as e:
        print(f"å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

        error_response = f"""âŒ **å¤„ç†å¤±è´¥**

é”™è¯¯ä¿¡æ¯: {str(e)}

è¯·æ£€æŸ¥æ‚¨çš„è¯·æ±‚å¹¶é‡è¯•ã€‚"""

        state["messages"].append(AIMessage(content=error_response))
        state["markdown_processed"] = True

    print("=== Enhanced Markdown to HTML Node å®Œæˆ ===")
    return state