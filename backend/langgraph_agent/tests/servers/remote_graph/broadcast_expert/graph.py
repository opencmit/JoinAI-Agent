"""
Broadcaståšå®¢å†™ä½œä¸“å®¶æ™ºèƒ½ä½“æœåŠ¡ - ä¸¤èŠ‚ç‚¹æ¶æ„ï¼ˆplanner + agentï¼‰
"""

import os
import sys
import asyncio
from typing import Annotated, AsyncIterator
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage,SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# æ·»åŠ toolsç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from tools.base_tools import get_current_time, search_web, read_file, write_file, word_count
from tools.broadcast_tools import analyze_seo, score_title, format_social_media


class State(TypedDict):
    """å›¾çŠ¶æ€å®šä¹‰"""
    messages: Annotated[list, add_messages]


def create_broadcast_planner():
    """åˆ›å»ºåšå®¢å†™ä½œè§„åˆ’èŠ‚ç‚¹"""
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹é…ç½®
    model_name = os.getenv("OPENAI_MODEL", "gpt-4")
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        raise ValueError("OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
    model_config = {"api_key": api_key}
    if base_url:
        model_config["base_url"] = base_url
        
    llm = init_chat_model(f"openai:{model_name}", **model_config)
    
    # åšå®¢å†™ä½œè§„åˆ’ç³»ç»Ÿæç¤ºè¯
    planner_prompt = """ä½ æ˜¯å†…å®¹è¥é”€è§„åˆ’åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æéœ€æ±‚å¹¶åˆ¶å®šä¼ æ’­ç­–ç•¥ã€‚

ã€è§„åˆ’ä»»åŠ¡ã€‘
1. åˆ†æå†…å®¹è¥é”€éœ€æ±‚å’Œç›®æ ‡
2. ç¡®å®šç›®æ ‡å—ä¼—å’Œä¼ æ’­æ¸ é“
3. è®¾è®¡å†…å®¹ç»“æ„å’ŒSEOç­–ç•¥
4. è§„åˆ’å†…å®¹é£æ ¼å’Œäº’åŠ¨è®¾è®¡
5. åˆ¶å®šå‘å¸ƒå’Œæ¨å¹¿æ–¹æ¡ˆ

ã€è¾“å‡ºæ ¼å¼ã€‘
ğŸ“‹ å†…å®¹è¥é”€è§„åˆ’æ–¹æ¡ˆï¼š

ã€è¥é”€ç›®æ ‡ã€‘
- å†…å®¹å®šä½ï¼š[ä¸»é¢˜è§’åº¦å’Œä»·å€¼ä¸»å¼ ]
- ä¼ æ’­ç›®æ ‡ï¼š[æœŸæœ›è¾¾æˆçš„æ•ˆæœ]

ã€å—ä¼—åˆ†æã€‘
- ç›®æ ‡äººç¾¤ï¼š[ç”¨æˆ·ç”»åƒ]
- ç—›ç‚¹éœ€æ±‚ï¼š[è§£å†³ä»€ä¹ˆé—®é¢˜]
- é˜…è¯»ä¹ æƒ¯ï¼š[å†…å®¹åå¥½]

ã€SEOç­–ç•¥ã€‘
- æ ¸å¿ƒå…³é”®è¯ï¼š[ä¸»è¦å…³é”®è¯]
- é•¿å°¾å…³é”®è¯ï¼š[ç›¸å…³è¯ç»„]
- å†…é“¾å¤–é“¾ï¼š[é“¾æ¥ç­–ç•¥]

ã€å†…å®¹æ¶æ„ã€‘
- æ ‡é¢˜ç­–ç•¥ï¼š[å¸å¼•åŠ›è®¾è®¡]
- æ®µè½ç»“æ„ï¼š[å†…å®¹ç»„ç»‡]
- è§†è§‰å…ƒç´ ï¼š[å›¾è¡¨/ä¿¡æ¯å›¾]

ã€ä¼ æ’­ç­–ç•¥ã€‘
- å‘å¸ƒæ¸ é“ï¼š[å¹³å°é€‰æ‹©]
- å‘å¸ƒæ—¶æœºï¼š[æœ€ä½³æ—¶é—´]
- äº’åŠ¨è®¾è®¡ï¼š[CTA/å¼•å¯¼]
- ç—…æ¯’å› å­ï¼š[åˆ†äº«åŠ¨æœº]

ã€æˆæ•ˆæŒ‡æ ‡ã€‘
- é˜…è¯»æŒ‡æ ‡ï¼š[æµè§ˆ/åœç•™]
- äº’åŠ¨æŒ‡æ ‡ï¼š[è¯„è®º/åˆ†äº«]
- è½¬åŒ–æŒ‡æ ‡ï¼š[æ³¨å†Œ/è´­ä¹°]"""

    async def planner_node(state: State) -> AsyncIterator[dict]:
        """åšå®¢è§„åˆ’èŠ‚ç‚¹ - å¼‚æ­¥æµå¼è¾“å‡º"""
        messages = state["messages"]
        
        # æ„å»ºè§„åˆ’æ¶ˆæ¯
        planning_messages = [SystemMessage(content=planner_prompt)]
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        if messages:
            planning_messages.extend(messages)
        
        # ä½¿ç”¨ astream è¿›è¡Œå¼‚æ­¥æµå¼è°ƒç”¨ï¼Œç´¯ç§¯å†…å®¹
        accumulated_content = ""
        async for chunk in llm.astream(planning_messages):
            # å¤„ç†æµå¼è¾“å‡ºçš„æ¯ä¸ªå—
            if hasattr(chunk, 'content') and chunk.content:
                accumulated_content += chunk.content
                # åªè¿”å›LLMç”Ÿæˆçš„å†…å®¹
                yield {"messages": [AIMessage(content=accumulated_content)]}
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ç”Ÿæˆï¼Œè¿”å›ç©ºæ¶ˆæ¯
        if not accumulated_content:
            yield {"messages": [AIMessage(content="")]}
    
    return planner_node


def create_broadcast_agent():
    """åˆ›å»ºåšå®¢å†™ä½œæ‰§è¡ŒèŠ‚ç‚¹"""
    
    # ä»ç¯å¢ƒå˜é‡è·å–æ¨¡å‹é…ç½®
    model_name = os.getenv("OPENAI_MODEL", "gpt-4")
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key:
        raise ValueError("OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
    
    # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
    model_config = {"api_key": api_key}
    if base_url:
        model_config["base_url"] = base_url
        
    llm = init_chat_model(f"openai:{model_name}", **model_config)
    
    # ç»‘å®šå·¥å…·åˆ°LLM
    tools = [
        get_current_time,
        search_web,
        read_file,
        write_file,
        word_count,
        analyze_seo,
        score_title,
        format_social_media
    ]
    llm_with_tools = llm.bind_tools(tools)
    
    # åšå®¢å†™ä½œæ‰§è¡Œç³»ç»Ÿæç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åšå®¢å†™ä½œä¸å†…å®¹è¥é”€ä¸“å®¶ï¼Œæ ¹æ®å‰é¢çš„è§„åˆ’æ–¹æ¡ˆåˆ›ä½œè¥é”€å†…å®¹ã€‚

ã€æ ¸å¿ƒä¸“é•¿ã€‘
ğŸ¯ å†…å®¹è¥é”€æˆ˜ç•¥
â€¢ ç›®æ ‡å—ä¼—åˆ†æä¸ç”¨æˆ·ç”»åƒæ„å»º
â€¢ å†…å®¹ä¸»é¢˜è§„åˆ’ä¸å‘å¸ƒæ—¥ç¨‹
â€¢ å“ç‰Œå£°éŸ³å®šä½ä¸è°ƒæ€§æŠŠæ§
â€¢ è·¨å¹³å°å†…å®¹é€‚é…ç­–ç•¥

ğŸ“ˆ SEOä¼˜åŒ–æŠ€èƒ½
â€¢ å…³é”®è¯ç ”ç©¶ä¸é•¿å°¾è¯æŒ–æ˜
â€¢ æ ‡é¢˜ä¼˜åŒ–ä¸å…ƒæ ‡ç­¾è®¾ç½®
â€¢ å†…é“¾å»ºè®¾ä¸å¤–é“¾ç­–ç•¥
â€¢ æŠ€æœ¯SEOä¸é¡µé¢æ€§èƒ½ä¼˜åŒ–
â€¢ Google Analyticsæ•°æ®åˆ†æ

âœï¸ å†™ä½œæŠ€å·§ç²¾é€š
â€¢ å¸å¼•çœ¼çƒçš„æ ‡é¢˜åˆ›ä½œ
â€¢ ç»“æ„åŒ–å†…å®¹ç»„ç»‡ï¼ˆAIDAã€PASæ¡†æ¶ï¼‰
â€¢ æ•…äº‹åŒ–å™è¿°ä¸æƒ…æ„Ÿå…±é¸£
â€¢ è¡ŒåŠ¨å¬å”¤(CTA)è®¾è®¡

ã€å¯ç”¨å·¥å…·ã€‘
ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥è¾…åŠ©å†…å®¹è¥é”€ï¼š
- get_current_time: è·å–å½“å‰æ—¶é—´
- search_web: æœç´¢ç½‘ç»œè·å–å‚è€ƒä¿¡æ¯
- read_file/write_file: è¯»å–æˆ–ä¿å­˜æ–‡æ¡£
- word_count: ç»Ÿè®¡å­—æ•°
- analyze_seo: åˆ†æSEOä¼˜åŒ–
- score_title: è¯„ä¼°æ ‡é¢˜å¸å¼•åŠ›
- format_social_media: æ ¼å¼åŒ–ç¤¾äº¤åª’ä½“å†…å®¹

ã€æ‰§è¡Œè¦æ±‚ã€‘
åŸºäºå‰é¢çš„è§„åˆ’æ–¹æ¡ˆï¼Œåˆ›ä½œé«˜è´¨é‡çš„åšå®¢å†…å®¹ï¼š
1. éµå¾ªSEOç­–ç•¥ï¼Œåˆç†å¸ƒå±€å…³é”®è¯
2. æŒ‰ç…§è§„åˆ’çš„ç»“æ„ç»„ç»‡å†…å®¹
3. ä¿æŒç›®æ ‡å—ä¼—å–œæ¬¢çš„é£æ ¼
4. è®¾è®¡æœ‰æ•ˆçš„äº’åŠ¨å…ƒç´ 
5. ç¡®ä¿å†…å®¹åŸåˆ›æ€§å’Œä»·å€¼æ€§

è¾“å‡ºå®Œæ•´çš„åšå®¢æ–‡ç« ï¼ŒåŒ…æ‹¬ï¼š
- å¸å¼•äººçš„æ ‡é¢˜
- ç»“æ„åŒ–çš„æ­£æ–‡å†…å®¹
- SEOä¼˜åŒ–å»ºè®®
- ç¤¾äº¤åª’ä½“æ¨å¹¿æ–‡æ¡ˆ
- å…³é”®è¯å’Œæ ‡ç­¾å»ºè®®"""

    async def agent_node(state: State) -> AsyncIterator[dict]:
        """åšå®¢å†™ä½œæ‰§è¡ŒèŠ‚ç‚¹ - å¼‚æ­¥æµå¼è¾“å‡º"""
        messages = state["messages"]
        
        # æ„å»ºæ‰§è¡Œæ¶ˆæ¯ï¼ˆåŒ…å«è§„åˆ’ä¿¡æ¯ï¼‰
        execution_messages = [SystemMessage(content=system_prompt)]
        
        # æ·»åŠ æ‰€æœ‰å†å²æ¶ˆæ¯ï¼ˆåŒ…æ‹¬è§„åˆ’ï¼‰
        if messages:
            execution_messages.extend(messages)
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ - è·å–å·¥å…·è°ƒç”¨å†³å®šï¼ˆéæµå¼ï¼Œå› ä¸ºéœ€è¦å®Œæ•´çš„å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼‰
        response = await llm_with_tools.ainvoke(execution_messages)
        
        # åˆå§‹åŒ–ç»“æœæ¶ˆæ¯åˆ—è¡¨
        result_messages = []
        
        # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # ä¿å­˜å¸¦æœ‰å·¥å…·è°ƒç”¨çš„ AIMessage
            result_messages.append(response)
            
            # æ‰§è¡Œå·¥å…·å¹¶åˆ›å»º ToolMessage
            for tool_call in response.tool_calls:
                tool_name = tool_call['name']
                tool_args = tool_call['args']
                tool_id = tool_call.get('id', f"call_{tool_name}")
                
                # æŸ¥æ‰¾å¹¶æ‰§è¡Œå¯¹åº”çš„å·¥å…·
                tool_func = None
                for tool in tools:
                    if tool.name == tool_name:
                        tool_func = tool
                        break
                
                if tool_func:
                    try:
                        # æ‰§è¡Œå·¥å…·
                        tool_result = tool_func.invoke(tool_args)
                        # åˆ›å»º ToolMessage
                        tool_message = ToolMessage(
                            content=str(tool_result),
                            tool_call_id=tool_id
                        )
                        result_messages.append(tool_message)
                    except Exception as e:
                        # é”™è¯¯ä¹Ÿéœ€è¦è¿”å› ToolMessage
                        tool_message = ToolMessage(
                            content=f"å·¥å…·æ‰§è¡Œé”™è¯¯: {str(e)}",
                            tool_call_id=tool_id
                        )
                        result_messages.append(tool_message)
            
            # ç¬¬äºŒæ¬¡è°ƒç”¨ - åŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå“åº”ï¼ˆå¯ä»¥æµå¼ï¼‰
            final_messages = execution_messages + result_messages
            
            # æµå¼ç”Ÿæˆæœ€ç»ˆå“åº”
            accumulated_content = ""
            async for chunk in llm.astream(final_messages):
                if hasattr(chunk, 'content') and chunk.content:
                    accumulated_content += chunk.content
                    # åˆ›å»ºä¸´æ—¶çš„ AIMessage ç”¨äºæµå¼è¾“å‡º
                    temp_final_message = AIMessage(content=accumulated_content)
                    # è¿”å›å®Œæ•´çš„æ¶ˆæ¯é“¾åŠ ä¸Šæ­£åœ¨ç”Ÿæˆçš„æœ€ç»ˆæ¶ˆæ¯
                    yield {"messages": result_messages + [temp_final_message]}
            
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆå†…å®¹ï¼Œæ·»åŠ ç©ºçš„æœ€ç»ˆæ¶ˆæ¯
            if not accumulated_content:
                result_messages.append(AIMessage(content=""))
                yield {"messages": result_messages}
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å“åº”
            result_messages.append(response)
            yield {"messages": result_messages}
    
    return agent_node


def create_broadcast_expert_graph():
    """åˆ›å»ºåšå®¢å†™ä½œä¸“å®¶æ™ºèƒ½ä½“å›¾ - ä¸¤èŠ‚ç‚¹æ¶æ„ï¼Œæ”¯æŒå¼‚æ­¥æµå¼è¾“å‡º"""
    
    # åˆ›å»ºçŠ¶æ€å›¾
    graph_builder = StateGraph(State)
    
    # åˆ›å»ºè§„åˆ’èŠ‚ç‚¹å’Œæ‰§è¡ŒèŠ‚ç‚¹
    broadcast_planner = create_broadcast_planner()
    broadcast_agent = create_broadcast_agent()
    
    # æ·»åŠ èŠ‚ç‚¹
    graph_builder.add_node("planner", broadcast_planner)
    graph_builder.add_node("agent", broadcast_agent)  # agentèŠ‚ç‚¹æ”¯æŒå¼‚æ­¥æµå¼è¾“å‡º
    
    # æ·»åŠ è¾¹ - ä¸¤èŠ‚ç‚¹æµç¨‹
    graph_builder.add_edge(START, "planner")
    graph_builder.add_edge("planner", "agent")
    graph_builder.add_edge("agent", END)
    
    # ç¼–è¯‘å›¾ - å¯ç”¨æµå¼æ”¯æŒ
    return graph_builder.compile()


# åˆ›å»ºå›¾å®ä¾‹ï¼ˆè¿™æ˜¯langgraph.jsonä¸­å¼•ç”¨çš„å…¥å£ç‚¹ï¼‰
broadcast_expert_graph = create_broadcast_expert_graph()